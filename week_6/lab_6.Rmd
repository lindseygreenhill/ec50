---
title: "lab_6"
author: "Lindsey Greenhill"
date: "3/31/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(haven)
library(rpart)
library(stargazer)
library(tidyverse)

mobility <- read_dta("mobility.dta")
set.seed(31305187)
```

## Question 1

We need to split our data into test and training datasets so we can develop a
model using the training set and then test that model on the test set and see
how well the model performs with out of sample predictions. This is a way to
test if the model is overfit to the data used to develop it.

## Question 2

```{r echo=FALSE}

#Uniformly distributed random number between 0 and 1
mobility$random_number <- runif(length(mobility$cz))

## Generate a training flag for 50% of the sample
mobility$train_flag <- ifelse(mobility$random_number>= 0.5, 1, 0) 

#Report number of observations in training and test samples
train_n <- sum(mobility$train_flag)
test_n <- sum(1-mobility$train_flag)
```

There are `r train_n` observations in the training set (treatament) and `r test_n` in the test set (control). 

## Question 3

```{r echo=FALSE}
# Create some data frames that just contain the training and test data
test <- subset(mobility, train_flag == 0)
train <- subset(mobility, train_flag == 1)
```

\newpage

## Question 4


```{r echo=FALSE, results = "asis"}

# part a: creating a model with 3 prediction variables

mod_3 <- lm(kfr_pooled_pooled_p25 ~ bowl_per_capita + singleparent_share1990 + 
              frac_coll_plus2000, data = train)

# summary(mod_3)

# Regression example: modify this code to complete the coding exercise

mobilityreg <- lm(kfr_pooled_pooled_p25 ~ bowl_per_capita + 
                    singleparent_share1990, data=train) 

stargazer(mod_3, mobilityreg,
          type = "latex")

# Display data for Milwaukee, WI

# summary(subset(mobility, cz == 24100))

MI <- mobility %>%
  filter(cz == 24100) %>%
  select(kfr_pooled_pooled_p25,
         bowl_per_capita,
         singleparent_share1990,
         frac_coll_plus2000)

MI_um <- 53.3898 + (.3475*5.72) - (68.2430*.226172) + (8.9554*.2515494)

```

\newpage

### Part a

The table above shows the regression results for the two variable regression in
the starter code and a modified regression with 3 variables. 

### Part b (check this)

- Using theregression coefficients from the 3 variable model, we can predict the
upward mobility rate in Milwaukee, WI using the equation y = 53.3898 +
.3475bowl_per_capita -68.2430singleparent_share1990 + 8.9554frac_coll_plus2000.
Using this equation, we can predict that Milwaukee has an upward mobility rate
of **`r MI_um`**.

- To calculate the prediction error, we subtract the prediction calculated
above from the actual value of Milwaukee's kfr_pooled_pooled_25 variable (
which is 38.88789). The prediction error = **`r 42.19557 - 38.88789`**

## Part c, d, e, f

```{r echo=FALSE}

#Generate predictions for all observations in the test data
y_test_predictions_ols <- predict(mod_3, newdata=test)

#Generate predictions for all observations in the training data
y_train_predictions_ols <- predict(mod_3, newdata=train)

#Generate squared prediction errors
OLS_performance_testset <- (test$kfr_pooled_pooled_p25 - y_test_predictions_ols)^2
OLS_performance_trainset <- (train$kfr_pooled_pooled_p25 - y_train_predictions_ols)^2

#Report the root mean squared prediction error

rmspe_test_ols <- sqrt(mean(OLS_performance_testset, na.rm=TRUE))

rmspe_train_ols <- sqrt(mean(OLS_performance_trainset, na.rm=TRUE))


```

- The root mean squared prediction error for the test data = `r rmspe_test_ols`.

- The root mean squared prediction error for the train data = `r rmspe_train_ols`.

- The rmspe for the test data is higher than the rmspe for the train data.
