---
title: "lab_6"
author: "Lindsey Greenhill"
date: "3/31/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(haven)
library(rpart)
library(stargazer)
library(tidyverse)

mobility <- read_dta("mobility.dta")
set.seed(31305187)
```

## Question 1

We need to split our data into test and training datasets so we can develop a
model using the training set and then test that model on the test set and see
how well the model performs with out of sample predictions. This is a way to
test if the model is overfit to the data used to develop it.

## Question 2

```{r echo=FALSE}

#Uniformly distributed random number between 0 and 1
mobility$random_number <- runif(length(mobility$cz))

## Generate a training flag for 50% of the sample
mobility$train_flag <- ifelse(mobility$random_number>= 0.5, 1, 0) 

#Report number of observations in training and test samples
train_n <- sum(mobility$train_flag)
test_n <- sum(1-mobility$train_flag)
```

There are `r train_n` observations in the training set (treatament) and `r test_n` in the test set (control). 

## Question 3

```{r echo=FALSE}
# Create some data frames that just contain the training and test data
test <- subset(mobility, train_flag == 0)
train <- subset(mobility, train_flag == 1)
```

\newpage

## Question 4


```{r echo=FALSE, results = "asis"}

# part a: creating a model with 3 prediction variables

mod_3 <- lm(kfr_pooled_pooled_p25 ~ bowl_per_capita + singleparent_share1990 + 
              frac_coll_plus2000, data = train)

# summary(mod_3)

# Regression example: modify this code to complete the coding exercise

mobilityreg <- lm(kfr_pooled_pooled_p25 ~ bowl_per_capita + 
                    singleparent_share1990, data=train) 

stargazer(mod_3, mobilityreg,
          type = "latex")

# Display data for Milwaukee, WI

# summary(subset(mobility, cz == 24100))

MI <- mobility %>%
  filter(cz == 24100) %>%
  select(kfr_pooled_pooled_p25,
         bowl_per_capita,
         singleparent_share1990,
         frac_coll_plus2000)

MI_um <- 52.84 + (.369*5.72) - (65.535*.226172) + (7.717*.2515494)

```

\newpage

### Part a

The table above shows the regression results for the two variable regression in
the starter code and a modified regression with 3 variables. 

### Part b (check this)

- Using theregression coefficients from the 3 variable model, we can predict the
upward mobility rate in Milwaukee, WI using the equation y = 52.84 +
.369bowl_per_capita -65.535singleparent_share1990 + 7.717frac_coll_plus2000.
Using this equation, we can predict that Milwaukee has an upward mobility rate
of **`r MI_um`**.

- To calculate the prediction error, we subtract the prediction calculated
above from the actual value of Milwaukee's kfr_pooled_pooled_25 variable (
which is 38.88789). The prediction error = **`r MI_um - 38.88789`**

### Part c, d, e, f

```{r echo=FALSE}

#Generate predictions for all observations in the test data
y_test_predictions_ols <- predict(mod_3, newdata=test)

#Generate predictions for all observations in the training data
y_train_predictions_ols <- predict(mod_3, newdata=train)

#Generate squared prediction errors
OLS_performance_testset <- (test$kfr_pooled_pooled_p25 - y_test_predictions_ols)^2
OLS_performance_trainset <- (train$kfr_pooled_pooled_p25 - y_train_predictions_ols)^2

#Report the root mean squared prediction error

rmspe_test_ols <- sqrt(mean(OLS_performance_testset, na.rm=TRUE))

rmspe_train_ols <- sqrt(mean(OLS_performance_trainset, na.rm=TRUE))


```

- The root mean squared prediction error for the test data = `r rmspe_test_ols`.

- The root mean squared prediction error for the train data = `r rmspe_train_ols`.

- The rmspe for the test data is higher than the rmspe for the train data.

## Question 5

```{r echo=FALSE}

#### Trees example: modify this code to complete the coding exercise
## Method is rpart()
## Depth 3
mobilitytree <- rpart(kfr_pooled_pooled_p25 ~ bowl_per_capita + singleparent_share1990 + frac_coll_plus2000, 
                      data=train, 
                      maxdepth = 3, 
                      cp=0) 

#Options for rpart
#cp = complexity parameter, which controls the complexity of the tree. 0 is most complex.
#If cp>0 the tree will only grown if the increase in tree size improve the performance of the tree by at least cp.
#maxdepth = maximum depth of any node of the final tree, with the root node counted as depth 0
#Other tuning parameters that can be changed
# help("rpart.control")

```
### Part b

To get our answer for Milwaulkee, we can go down the tree and follow the
branches based on the values for different variables. Single parent share is >
.15, so we go to the right of the tree, then, bowl_per_capita is less than 18.37
and greater than 5.584, so we end up at 52.18 as a prediction. The actual
kfr_pooled_pooled_p25 = 38.88. As such, the prediction error = 52.18 - 38.88 =
13.3

```{r echo=FALSE}
#Visualize the fitted decision tree
plot(mobilitytree, margin = 0.2)
text(mobilitytree, cex = 0.5)

# apply to WI. To get our answer for Milwaulkee, we can go down the tree and
# follow the branches based on the values for different variables. Single parent
# share is > .15, so we go to the right of the tree, then, bowl_per_capita is
# less than 18.37 and greater than 5.584, so we end up at 52.18 as a prediction.
# The actual kfr_pooled_pooled_p25 = 38.88. As such, the prediction error =
# 52.18 - 38.88 = 13.3

```

### Parts c, d, e, f

```{r echo=FALSE}

#Calculate predictions for all rows in test and training samples
y_test_predictions_tree <- predict(mobilitytree, newdata=test)
y_train_predictions_tree <- predict(mobilitytree, newdata=train)

#Generate squared prediction errors
tree_performance_testset <- (test$kfr_pooled_pooled_p25 - y_test_predictions_tree)^2
tree_performance_trainset <- (train$kfr_pooled_pooled_p25 - y_train_predictions_tree)^2

#Report the root mean squared prediction error
rmspe_test_tree <- sqrt(mean(tree_performance_testset, na.rm=TRUE))
rmspe_train_tree <- sqrt(mean(tree_performance_trainset, na.rm=TRUE))


```

- The rmspe for the test data = `r rmspe_test_tree`

- The rmspe for the train data = `r rmspe_train_tree`

- The rmspe is higher for the test data than for the training data. 

## Question 6

```{r echo=FALSE}
#Estimate large tree
big_tree <-rpart(kfr_pooled_pooled_p25 ~ bowl_per_capita + singleparent_share1990 +
                   frac_coll_plus2000, 
             data=train, 
             maxdepth = 30, 
             cp=0,  
             minsplit = 1, 
             minbucket = 1)

#Visualize the fitted decision tree
{
plot(big_tree, margin = 0.2)
text(big_tree, cex = 0.5)
}

#Calculate predictions for all rows in test and training samples
y_test_predictions_big_tree <- predict(big_tree, newdata=test)
y_train_predictions_big_tree <- predict(big_tree, newdata=train)

#Generate squared prediction errors
big_tree_performance_testset <- (test$kfr_pooled_pooled_p25 - y_test_predictions_big_tree)^2
big_tree_performance_trainset <- (train$kfr_pooled_pooled_p25 - y_train_predictions_big_tree)^2

#Report the root mean squared prediction error
rmspe_test_big_tree <- sqrt(mean(big_tree_performance_testset, na.rm=TRUE))
rmspe_train_big_tree <- sqrt(mean(big_tree_performance_trainset, na.rm=TRUE))

```

- The rmspe for the test data = `r rmspe_test_big_tree`

- The rmspe for the training data = 0.

- Obviously, the rmspe is larger for the test data than the training data. 

## Question 7

- Training sample: When comparing the rmspe for the three models on the training
data set, The big decision tree performs best followed by the small decision
tree and the regression, respectively.

- Test sampleL when comparing the rmspe for the three models on the test data set,



